# 🚀 **RENDER.COM DEPLOYMENT GUIDE**
## Complete MCP Server Deployment with Enhanced AI Providers

*Updated for Cerebras, Groq, Mistral integration + External Database Services*

---

## 🎯 **DEPLOYMENT ARCHITECTURE**

```ascii
                    🌐 RENDER.COM INFRASTRUCTURE
                              │
                 ┌────────────┼────────────┐
                 │            │            │
                 ▼            ▼            ▼
        ┌─────────────┐ ┌─────────────┐ ┌─────────────┐
        │ 🚀 MCP      │ │ 📊 External │ │ 🔧 External │
        │ Server      │ │ Databases   │ │ Services    │
        │ (Render)    │ │             │ │             │
        └─────────────┘ └─────────────┘ └─────────────┘
                │              │              │
                │              │              │
        ┌───────▼──────┐ ┌─────▼─────┐ ┌─────▼─────┐
        │ Web Service  │ │ Supabase  │ │ Redis     │
        │ Standard     │ │ PostgreSQL│ │ Cloud     │
        │ Plan         │ │ + Vector  │ │ Cache     │
        └──────────────┘ └───────────┘ └───────────┘
                │              │              │
                └──────────────┼──────────────┘
                               │
                      ┌────────▼────────┐
                      │ 🤖 AI Providers │
                      │ • Cerebras      │
                      │ • Groq          │
                      │ • Mistral       │
                      │ • NVIDIA        │
                      │ • Gemini        │
                      │ • Pinecone      │
                      └─────────────────┘
```

---

## 📋 **PRE-DEPLOYMENT CHECKLIST**

### **✅ Required External Services**

1. **📊 Supabase Account**: https://supabase.com
   - Create new project
   - Get URL and service role key
   - Enable vector extension

2. **🔴 Redis Cloud Account**: https://redis.com
   - Create free Redis instance
   - Get connection URL and API key

3. **🧠 Pinecone Account**: https://pinecone.io
   - Create vector database index
   - Get API key

4. **🤖 AI Provider API Keys**:
   - ✅ **Cerebras**: https://cerebras.ai (PRIORITY)
   - ✅ **Groq**: https://groq.com (WORKING)
   - ✅ **Mistral**: https://mistral.ai (WORKING)
   - 🔄 **NVIDIA**: https://nvidia.com/ai (OPTIONAL)
   - ✅ **Google AI**: https://makersuite.google.com (EXISTING)

---

## 🚀 **STEP-BY-STEP DEPLOYMENT**

### **Step 1: Prepare Repository**

```bash
# 1. Ensure all files are in the correct structure
git add .
git commit -m "🚀 Deploy MCP Server v3.0 with enhanced AI providers"
git push origin main

# 2. Verify render.yaml is in root
cp /workspaces/mcpserver/render/render.yaml ./render.yaml
```

### **Step 2: Connect to Render**

1. **🌐 Go to Render Dashboard**: https://dashboard.render.com
2. **➕ New → Web Service**
3. **📁 Connect Repository**: Link your GitHub repository
4. **⚙️ Auto-Detection**: Render will detect `render.yaml`

### **Step 3: Configure Environment Variables**

**🔑 In Render Dashboard → Environment Variables:**

```bash
# Core Configuration
NODE_ENV=production
PORT=3002
LOG_LEVEL=info

# Database (Supabase)
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key_here

# AI Providers (🏆 PRIORITY ORDER)
CEREBRAS_API_KEY=csk-your_cerebras_key_here
GROQ_API_KEY=gsk_your_groq_key_here
MINSTRAL_API_KEY=your_mistral_key_here
NVIDIA_API_KEY=your_nvidia_key_here
GEMINI_API_KEY=your_gemini_key_here
OPENROUTER_API_KEY=sk-or-v1-your_openrouter_key
COHERE_API_KEY=your_cohere_key_here

# Vector Database
PINECONE_API_KEY=your_pinecone_key_here

# Cache (Redis Cloud)
REDIS_URL=redis://default:password@host:port
REDIS_API_KEY=your_redis_api_key

# Security (Auto-generated by Render)
JWT_SECRET=auto_generated_by_render
MCP_AUTH_TOKEN=auto_generated_by_render

# Configuration
ALLOWED_ORIGINS=https://your-frontend.vercel.app,https://localhost:3000
MAX_REQUESTS_PER_MINUTE=100
FILESYSTEM_READONLY=false
MAX_FILE_SIZE=50000000
```

### **Step 4: Deploy and Monitor**

```bash
# 1. Trigger deployment
# Render will automatically build and deploy

# 2. Monitor build logs
# Watch for successful npm install and build

# 3. Check health endpoint
curl https://your-mcp-server.onrender.com/health

# 4. Verify AI providers
curl https://your-mcp-server.onrender.com/health | jq '.aiProviders'
```

---

## 🔧 **RENDER CONFIGURATION DETAILS**

### **📄 render.yaml Breakdown**

```yaml
services:
  - type: web
    name: charnoks-mcp-server
    env: node
    plan: standard  # $25/month for better performance
    region: oregon  # Closest to AI providers
    buildCommand: npm install && npm run build
    startCommand: npm start
    healthCheckPath: /health
    
    # 💾 Persistent storage for logs and data
    disk:
      name: mcp-disk
      sizeGB: 10
      mountPath: /app/data
```

### **🎯 Service Plan Recommendations**

| Plan | Price | CPU | RAM | Use Case |
|------|-------|-----|-----|----------|
| **Starter** | $7/month | 0.5 CPU | 512MB | Development |
| **Standard** | $25/month | 1 CPU | 2GB | **RECOMMENDED** |
| **Pro** | $85/month | 2 CPU | 4GB | High traffic |

### **🌍 Region Selection**

- **Oregon (us-west)**: Best for AI provider latency
- **Ohio (us-east)**: Good for database connections
- **Frankfurt**: If targeting European users

---

## 📊 **POST-DEPLOYMENT VERIFICATION**

### **🔍 Health Check Verification**

```bash
# Test main health endpoint
curl https://your-mcp-server.onrender.com/health

# Expected response structure:
{
  "status": "healthy",
  "version": "3.0.0",
  "services": {
    "gemini": { "overall": "healthy" },
    "supabase": { "status": "healthy" },
    "enhancedAI": {
      "totalProviders": 5,
      "availableProviders": 4,
      "providers": [
        { "name": "Cerebras", "available": true },
        { "name": "Groq", "available": true },
        { "name": "Mistral", "available": true }
      ]
    }
  }
}
```

### **🧪 Test AI Provider Integration**

```bash
# Test note processing with new AI
curl -X POST https://your-mcp-server.onrender.com/api/tools/call \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your_token" \
  -d '{
    "name": "parse_chicken_note",
    "arguments": {
      "content": "Fed 50 chickens, collected 35 eggs, sold 20 for $40",
      "branch_id": "main",
      "author_id": "test-user"
    }
  }'
```

### **⚡ Performance Verification**

```bash
# Test response times
time curl https://your-mcp-server.onrender.com/health

# Monitor logs
# Go to Render Dashboard → Logs tab
# Look for AI provider selection and response times
```

---

## 🛠️ **TROUBLESHOOTING GUIDE**

### **❌ Common Issues & Solutions**

#### **Build Failures**

```bash
# Issue: TypeScript compilation errors
# Solution: Check for missing dependencies
npm install @cerebras/cerebras_cloud_sdk

# Issue: Memory errors during build
# Solution: Upgrade to Standard plan
```

#### **Runtime Errors**

```bash
# Issue: AI provider not available
# Check: Environment variables set correctly
# Check: API keys have sufficient quota

# Issue: Database connection failed
# Check: Supabase URL and key are correct
# Check: Database is not paused (free tier)
```

#### **Performance Issues**

```bash
# Issue: Slow response times
# Solution 1: Upgrade to Standard plan
# Solution 2: Check AI provider latency
# Solution 3: Monitor Redis cache hit rates
```

### **📋 Debug Commands**

```bash
# View environment variables (in Render shell)
env | grep -E "(CEREBRAS|GROQ|MISTRAL)"

# Test AI provider directly
node -e "console.log(process.env.CEREBRAS_API_KEY ? 'OK' : 'MISSING')"

# Check disk usage
df -h /app/data

# Memory usage
free -h
```

---

## 🎯 **OPTIMIZATION RECOMMENDATIONS**

### **🚀 Performance Optimizations**

1. **AI Provider Failover**:
   ```javascript
   // Automatic fallback from Cerebras → Groq → Mistral
   // Already implemented in enhanced-multi-provider-ai.ts
   ```

2. **Redis Caching**:
   ```javascript
   // Cache AI responses for repeated queries
   // Reduces API costs and improves response times
   ```

3. **Load Balancing**:
   ```javascript
   // Distribute requests across multiple AI providers
   // Prevents rate limit exhaustion
   ```

### **💰 Cost Optimization**

| Provider | Cost/Token | Best For | Rate Limit |
|----------|------------|----------|------------|
| **Cerebras** | $0.000001 | Real-time, speed | 1000 RPM |
| **Groq** | $0.0000005 | Quick responses | 30 RPM |
| **Mistral** | $0.000008 | Complex analysis | 60 RPM |

**💡 Strategy**: Use Cerebras for primary requests, Groq for quick queries, Mistral for complex analysis.

---

## 📈 **MONITORING & SCALING**

### **📊 Key Metrics to Monitor**

1. **Response Times**: < 2 seconds target
2. **AI Provider Availability**: > 95% uptime
3. **Error Rate**: < 1% target
4. **Memory Usage**: < 80% of allocated
5. **Database Connections**: Monitor pool usage

### **🔔 Alerting Setup**

```bash
# Use Render's built-in monitoring
# Set up alerts for:
# - Service down (health check fails)
# - High error rate (> 5%)
# - Memory usage > 90%
```

### **📈 Scaling Strategy**

```yaml
# Horizontal scaling with multiple regions
services:
  - name: mcp-server-us-west
    region: oregon
  - name: mcp-server-us-east  
    region: ohio
    
# Load balancer configuration
# Route traffic based on geographic location
```

---

## 🎉 **DEPLOYMENT SUCCESS CHECKLIST**

- ✅ **Service Status**: Green in Render dashboard
- ✅ **Health Check**: Returns HTTP 200
- ✅ **AI Providers**: Cerebras, Groq, Mistral available
- ✅ **Database**: Supabase connection working
- ✅ **Cache**: Redis connection established
- ✅ **Vector DB**: Pinecone integration active
- ✅ **Authentication**: JWT tokens working
- ✅ **CORS**: Frontend can connect
- ✅ **Rate Limiting**: Protection active
- ✅ **Logs**: No critical errors
- ✅ **Performance**: Response times < 2s

---

## 📞 **SUPPORT & NEXT STEPS**

### **🔗 Useful Links**

- **Render Docs**: https://render.com/docs
- **Supabase Docs**: https://supabase.com/docs
- **Redis Cloud**: https://redis.com/cloud
- **Cerebras API**: https://cerebras.ai/api
- **Groq API**: https://groq.com/api

### **🚀 Next Steps After Deployment**

1. **Connect Frontend**: Update frontend environment variables
2. **Set up Monitoring**: Configure alerts and dashboards
3. **Scale Testing**: Test with production load
4. **Documentation**: Update API documentation
5. **Backup Strategy**: Implement database backups

---

**🎊 Congratulations! Your MCP Server with enhanced AI providers is now live on Render!**

*Your production URL: `https://your-mcp-server.onrender.com`*