# ğŸš€ **RENDER.COM DEPLOYMENT GUIDE**
## Complete MCP Server Deployment with Enhanced AI Providers

*Updated for Cerebras, Groq, Mistral integration + External Database Services*

---

## ğŸ¯ **DEPLOYMENT ARCHITECTURE**

```ascii
                    ğŸŒ RENDER.COM INFRASTRUCTURE
                              â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚            â”‚            â”‚
                 â–¼            â–¼            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ ğŸš€ MCP      â”‚ â”‚ ğŸ“Š External â”‚ â”‚ ğŸ”§ External â”‚
        â”‚ Server      â”‚ â”‚ Databases   â”‚ â”‚ Services    â”‚
        â”‚ (Render)    â”‚ â”‚             â”‚ â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚              â”‚              â”‚
                â”‚              â”‚              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
        â”‚ Web Service  â”‚ â”‚ Supabase  â”‚ â”‚ Redis     â”‚
        â”‚ Standard     â”‚ â”‚ PostgreSQLâ”‚ â”‚ Cloud     â”‚
        â”‚ Plan         â”‚ â”‚ + Vector  â”‚ â”‚ Cache     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚              â”‚              â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚ ğŸ¤– AI Providers â”‚
                      â”‚ â€¢ Cerebras      â”‚
                      â”‚ â€¢ Groq          â”‚
                      â”‚ â€¢ Mistral       â”‚
                      â”‚ â€¢ NVIDIA        â”‚
                      â”‚ â€¢ Gemini        â”‚
                      â”‚ â€¢ Pinecone      â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ **PRE-DEPLOYMENT CHECKLIST**

### **âœ… Required External Services**

1. **ğŸ“Š Supabase Account**: https://supabase.com
   - Create new project
   - Get URL and service role key
   - Enable vector extension

2. **ğŸ”´ Redis Cloud Account**: https://redis.com
   - Create free Redis instance
   - Get connection URL and API key

3. **ğŸ§  Pinecone Account**: https://pinecone.io
   - Create vector database index
   - Get API key

4. **ğŸ¤– AI Provider API Keys**:
   - âœ… **Cerebras**: https://cerebras.ai (PRIORITY)
   - âœ… **Groq**: https://groq.com (WORKING)
   - âœ… **Mistral**: https://mistral.ai (WORKING)
   - ğŸ”„ **NVIDIA**: https://nvidia.com/ai (OPTIONAL)
   - âœ… **Google AI**: https://makersuite.google.com (EXISTING)

---

## ğŸš€ **STEP-BY-STEP DEPLOYMENT**

### **Step 1: Prepare Repository**

```bash
# 1. Ensure all files are in the correct structure
git add .
git commit -m "ğŸš€ Deploy MCP Server v3.0 with enhanced AI providers"
git push origin main

# 2. Verify render.yaml is in root
cp /workspaces/mcpserver/render/render.yaml ./render.yaml
```

### **Step 2: Connect to Render**

1. **ğŸŒ Go to Render Dashboard**: https://dashboard.render.com
2. **â• New â†’ Web Service**
3. **ğŸ“ Connect Repository**: Link your GitHub repository
4. **âš™ï¸ Auto-Detection**: Render will detect `render.yaml`

### **Step 3: Configure Environment Variables**

**ğŸ”‘ In Render Dashboard â†’ Environment Variables:**

```bash
# Core Configuration
NODE_ENV=production
PORT=3002
LOG_LEVEL=info

# Database (Supabase)
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key_here

# AI Providers (ğŸ† PRIORITY ORDER)
CEREBRAS_API_KEY=csk-your_cerebras_key_here
GROQ_API_KEY=gsk_your_groq_key_here
MINSTRAL_API_KEY=your_mistral_key_here
NVIDIA_API_KEY=your_nvidia_key_here
GEMINI_API_KEY=your_gemini_key_here
OPENROUTER_API_KEY=sk-or-v1-your_openrouter_key
COHERE_API_KEY=your_cohere_key_here

# Vector Database
PINECONE_API_KEY=your_pinecone_key_here

# Cache (Redis Cloud)
REDIS_URL=redis://default:password@host:port
REDIS_API_KEY=your_redis_api_key

# Security (Auto-generated by Render)
JWT_SECRET=auto_generated_by_render
MCP_AUTH_TOKEN=auto_generated_by_render

# Configuration
ALLOWED_ORIGINS=https://your-frontend.vercel.app,https://localhost:3000
MAX_REQUESTS_PER_MINUTE=100
FILESYSTEM_READONLY=false
MAX_FILE_SIZE=50000000
```

### **Step 4: Deploy and Monitor**

```bash
# 1. Trigger deployment
# Render will automatically build and deploy

# 2. Monitor build logs
# Watch for successful npm install and build

# 3. Check health endpoint
curl https://your-mcp-server.onrender.com/health

# 4. Verify AI providers
curl https://your-mcp-server.onrender.com/health | jq '.aiProviders'
```

---

## ğŸ”§ **RENDER CONFIGURATION DETAILS**

### **ğŸ“„ render.yaml Breakdown**

```yaml
services:
  - type: web
    name: charnoks-mcp-server
    env: node
    plan: standard  # $25/month for better performance
    region: oregon  # Closest to AI providers
    buildCommand: npm install && npm run build
    startCommand: npm start
    healthCheckPath: /health
    
    # ğŸ’¾ Persistent storage for logs and data
    disk:
      name: mcp-disk
      sizeGB: 10
      mountPath: /app/data
```

### **ğŸ¯ Service Plan Recommendations**

| Plan | Price | CPU | RAM | Use Case |
|------|-------|-----|-----|----------|
| **Starter** | $7/month | 0.5 CPU | 512MB | Development |
| **Standard** | $25/month | 1 CPU | 2GB | **RECOMMENDED** |
| **Pro** | $85/month | 2 CPU | 4GB | High traffic |

### **ğŸŒ Region Selection**

- **Oregon (us-west)**: Best for AI provider latency
- **Ohio (us-east)**: Good for database connections
- **Frankfurt**: If targeting European users

---

## ğŸ“Š **POST-DEPLOYMENT VERIFICATION**

### **ğŸ” Health Check Verification**

```bash
# Test main health endpoint
curl https://your-mcp-server.onrender.com/health

# Expected response structure:
{
  "status": "healthy",
  "version": "3.0.0",
  "services": {
    "gemini": { "overall": "healthy" },
    "supabase": { "status": "healthy" },
    "enhancedAI": {
      "totalProviders": 5,
      "availableProviders": 4,
      "providers": [
        { "name": "Cerebras", "available": true },
        { "name": "Groq", "available": true },
        { "name": "Mistral", "available": true }
      ]
    }
  }
}
```

### **ğŸ§ª Test AI Provider Integration**

```bash
# Test note processing with new AI
curl -X POST https://your-mcp-server.onrender.com/api/tools/call \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your_token" \
  -d '{
    "name": "parse_chicken_note",
    "arguments": {
      "content": "Fed 50 chickens, collected 35 eggs, sold 20 for $40",
      "branch_id": "main",
      "author_id": "test-user"
    }
  }'
```

### **âš¡ Performance Verification**

```bash
# Test response times
time curl https://your-mcp-server.onrender.com/health

# Monitor logs
# Go to Render Dashboard â†’ Logs tab
# Look for AI provider selection and response times
```

---

## ğŸ› ï¸ **TROUBLESHOOTING GUIDE**

### **âŒ Common Issues & Solutions**

#### **Build Failures**

```bash
# Issue: TypeScript compilation errors
# Solution: Check for missing dependencies
npm install @cerebras/cerebras_cloud_sdk

# Issue: Memory errors during build
# Solution: Upgrade to Standard plan
```

#### **Runtime Errors**

```bash
# Issue: AI provider not available
# Check: Environment variables set correctly
# Check: API keys have sufficient quota

# Issue: Database connection failed
# Check: Supabase URL and key are correct
# Check: Database is not paused (free tier)
```

#### **Performance Issues**

```bash
# Issue: Slow response times
# Solution 1: Upgrade to Standard plan
# Solution 2: Check AI provider latency
# Solution 3: Monitor Redis cache hit rates
```

### **ğŸ“‹ Debug Commands**

```bash
# View environment variables (in Render shell)
env | grep -E "(CEREBRAS|GROQ|MISTRAL)"

# Test AI provider directly
node -e "console.log(process.env.CEREBRAS_API_KEY ? 'OK' : 'MISSING')"

# Check disk usage
df -h /app/data

# Memory usage
free -h
```

---

## ğŸ¯ **OPTIMIZATION RECOMMENDATIONS**

### **ğŸš€ Performance Optimizations**

1. **AI Provider Failover**:
   ```javascript
   // Automatic fallback from Cerebras â†’ Groq â†’ Mistral
   // Already implemented in enhanced-multi-provider-ai.ts
   ```

2. **Redis Caching**:
   ```javascript
   // Cache AI responses for repeated queries
   // Reduces API costs and improves response times
   ```

3. **Load Balancing**:
   ```javascript
   // Distribute requests across multiple AI providers
   // Prevents rate limit exhaustion
   ```

### **ğŸ’° Cost Optimization**

| Provider | Cost/Token | Best For | Rate Limit |
|----------|------------|----------|------------|
| **Cerebras** | $0.000001 | Real-time, speed | 1000 RPM |
| **Groq** | $0.0000005 | Quick responses | 30 RPM |
| **Mistral** | $0.000008 | Complex analysis | 60 RPM |

**ğŸ’¡ Strategy**: Use Cerebras for primary requests, Groq for quick queries, Mistral for complex analysis.

---

## ğŸ“ˆ **MONITORING & SCALING**

### **ğŸ“Š Key Metrics to Monitor**

1. **Response Times**: < 2 seconds target
2. **AI Provider Availability**: > 95% uptime
3. **Error Rate**: < 1% target
4. **Memory Usage**: < 80% of allocated
5. **Database Connections**: Monitor pool usage

### **ğŸ”” Alerting Setup**

```bash
# Use Render's built-in monitoring
# Set up alerts for:
# - Service down (health check fails)
# - High error rate (> 5%)
# - Memory usage > 90%
```

### **ğŸ“ˆ Scaling Strategy**

```yaml
# Horizontal scaling with multiple regions
services:
  - name: mcp-server-us-west
    region: oregon
  - name: mcp-server-us-east  
    region: ohio
    
# Load balancer configuration
# Route traffic based on geographic location
```

---

## ğŸ‰ **DEPLOYMENT SUCCESS CHECKLIST**

- âœ… **Service Status**: Green in Render dashboard
- âœ… **Health Check**: Returns HTTP 200
- âœ… **AI Providers**: Cerebras, Groq, Mistral available
- âœ… **Database**: Supabase connection working
- âœ… **Cache**: Redis connection established
- âœ… **Vector DB**: Pinecone integration active
- âœ… **Authentication**: JWT tokens working
- âœ… **CORS**: Frontend can connect
- âœ… **Rate Limiting**: Protection active
- âœ… **Logs**: No critical errors
- âœ… **Performance**: Response times < 2s

---

## ğŸ“ **SUPPORT & NEXT STEPS**

### **ğŸ”— Useful Links**

- **Render Docs**: https://render.com/docs
- **Supabase Docs**: https://supabase.com/docs
- **Redis Cloud**: https://redis.com/cloud
- **Cerebras API**: https://cerebras.ai/api
- **Groq API**: https://groq.com/api

### **ğŸš€ Next Steps After Deployment**

1. **Connect Frontend**: Update frontend environment variables
2. **Set up Monitoring**: Configure alerts and dashboards
3. **Scale Testing**: Test with production load
4. **Documentation**: Update API documentation
5. **Backup Strategy**: Implement database backups

---

**ğŸŠ Congratulations! Your MCP Server with enhanced AI providers is now live on Render!**

*Your production URL: `https://your-mcp-server.onrender.com`*